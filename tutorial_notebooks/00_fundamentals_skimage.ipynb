{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import supplementary_code as sc\n",
    "\n",
    "from ipywidgets import interact\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats\n",
    "\n",
    "from skimage import (color, data, exposure, feature, filters, io, measure,\n",
    "                    morphology, restoration, segmentation, transform,\n",
    "                    util)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-image: a tour\n",
    "\n",
    "There are many tools and utilities in the package, far too many to cover in a tutorial. This notebook is designed as a \n",
    "road map, to guide you as you explore or search for additional tools for your applications.\n",
    "\n",
    "Each submodule of scikit-image has its own section, which you can navigate to below:\n",
    "\n",
    "* [skimage.color](#color)\n",
    "* [skimage.data](#data)\n",
    "* [skimage.draw](#draw)\n",
    "* [skimage.exposure](#exposure)\n",
    "* [skimage.feature](#feature)\n",
    "* [skimage.filters](#filters)\n",
    "* [skimage.future](#future)\n",
    "* [skimage.graph](#graph)\n",
    "* [skimage.io](#io)\n",
    "* [skimage.measure](#measure)\n",
    "* [skimage.morphology](#morphology)\n",
    "* [skimage.restoration](#restoration)\n",
    "* [skimage.segmentation](#segmentation)\n",
    "* [skimage.transform](#transform)\n",
    "* [skimage.util](#util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.color](https://scikit-image.org/docs/stable/api/skimage.color.html) - color conversion<a id='color'></a>\n",
    "\n",
    "The `color` submodule includes routines to convert to and from common color representations.  For example, RGB (Red, Green, and Blue) can be converted into many other representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tab complete to see available functions in the color submodule\n",
    "color.rgb2\n",
    "color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: conversion to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = data.astronaut()\n",
    "grayscale = color.rgb2gray(original)\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(original)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(grayscale, cmap='gray')\n",
    "ax[1].set_title(\"Grayscale\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: conversion to HSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, objects in images have distinct colors (hues) and luminosities, so that these features can be used to separate different areas of the image. In the RGB representation the hue and the luminosity are expressed as a linear combination of the R,G,B channels, whereas they correspond to single channels of the HSV image (the Hue and the Value channels). A simple segmentation of the image can then be effectively performed by a mere thresholding of the HSV channels.  See below link for additional details.\n",
    "\n",
    "https://en.wikipedia.org/wiki/HSL_and_HSV\n",
    "\n",
    "We first load the RGB image and extract the Hue and Value channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.color import rgb2hsv\n",
    "\n",
    "rgb_img = data.coffee()\n",
    "hsv_img = rgb2hsv(rgb_img)\n",
    "hue_img = hsv_img[:, :, 0]\n",
    "value_img = hsv_img[:, :, 2]\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(8, 2))\n",
    "\n",
    "ax0.imshow(rgb_img)\n",
    "ax0.set_title(\"RGB image\")\n",
    "ax0.axis('off')\n",
    "ax1.imshow(hue_img, cmap='hsv')\n",
    "ax1.set_title(\"Hue channel\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(value_img)\n",
    "ax2.set_title(\"Value channel\")\n",
    "ax2.axis('off')\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cup and saucer have a Hue distinct from the remainder of the image, which can be isolated by thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_threshold = 0.04\n",
    "binary_img = hue_img > hue_threshold\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "\n",
    "ax0.hist(hue_img.ravel(), 512)\n",
    "ax0.set_title(\"Histogram of the Hue channel with threshold\")\n",
    "ax0.axvline(x=hue_threshold, color='r', linestyle='dashed', linewidth=2)\n",
    "ax0.set_xbound(0, 0.12)\n",
    "ax1.imshow(binary_img)\n",
    "ax1.set_title(\"Hue-thresholded image\")\n",
    "ax1.axis('off')\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional threshold in the value channel can remote most of the shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "value_threshold = 0.10\n",
    "binary_img = (hue_img > hue_threshold) | (value_img < value_threshold)\n",
    "\n",
    "ax0.imshow(binary_img)\n",
    "ax0.set_title(\"Hue and value thresholded image\")\n",
    "ax0.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional color conversion examples available in the [online gallery](https://scikit-image.org/docs/stable/auto_examples/#manipulating-exposure-and-color-channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.data](https://scikit-image.org/docs/stable/api/skimage.data.html) - test images<a id='data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` submodule includes standard test images useful for examples and testing the package.  These images are shipped with the package.\n",
    "\n",
    "There are scientific images, general test images, and a stereoscopic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "example_image = data.camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(example_image)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.draw](https://scikit-image.org/docs/stable/api/skimage.draw.html) - drawing primitives on an image<a id='draw'></a>\n",
    "\n",
    "The majority of functions in this submodule return the *coordinates* of the specified shape/object in the image, rather than drawing it on the image directly.  The coordinates can then be used as a mask to draw on the image, or you pass the image as well as those coordinates into the convenience function `draw.set_color`.\n",
    "\n",
    "Lines and circles can be drawn with antialiasing (these functions end in the suffix *_aa).\n",
    "\n",
    "At the current time text is not supported; other libraries including matplotlib have robust support for overlaying text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tab complete to see available options\n",
    "draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: drawing shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10, 6))\n",
    "\n",
    "img = np.zeros((500, 500, 3), dtype=np.float64)\n",
    "\n",
    "# draw line\n",
    "rr, cc = draw.line(120, 123, 20, 400)\n",
    "img[rr, cc, 0] = 255\n",
    "\n",
    "# fill polygon\n",
    "poly = np.array((\n",
    "    (300, 300),\n",
    "    (480, 320),\n",
    "    (380, 430),\n",
    "    (220, 590),\n",
    "    (300, 300),\n",
    "))\n",
    "rr, cc = draw.polygon(poly[:, 0], poly[:, 1], img.shape)\n",
    "img[rr, cc, 1] = 1\n",
    "\n",
    "# fill circle\n",
    "rr, cc = draw.circle(200, 200, 100, img.shape)\n",
    "img[rr, cc, :] = (1, 1, 0)\n",
    "\n",
    "# fill ellipse\n",
    "rr, cc = draw.ellipse(300, 300, 100, 200, img.shape)\n",
    "img[rr, cc, 2] = 1\n",
    "\n",
    "# circle\n",
    "rr, cc = draw.circle_perimeter(120, 400, 15)\n",
    "img[rr, cc, :] = (1, 0, 0)\n",
    "\n",
    "# Bezier curve\n",
    "rr, cc = draw.bezier_curve(70, 100, 10, 10, 150, 100, 1)\n",
    "img[rr, cc, :] = (1, 0, 0)\n",
    "\n",
    "# ellipses\n",
    "rr, cc = draw.ellipse_perimeter(120, 400, 60, 20, orientation=np.pi / 4.)\n",
    "img[rr, cc, :] = (1, 0, 1)\n",
    "rr, cc = draw.ellipse_perimeter(120, 400, 60, 20, orientation=-np.pi / 4.)\n",
    "img[rr, cc, :] = (0, 0, 1)\n",
    "rr, cc = draw.ellipse_perimeter(120, 400, 60, 20, orientation=np.pi / 2.)\n",
    "img[rr, cc, :] = (1, 1, 1)\n",
    "\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('No anti-aliasing')\n",
    "ax1.axis('off')\n",
    "\n",
    "\n",
    "img = np.zeros((100, 100), dtype=np.double)\n",
    "\n",
    "# anti-aliased line\n",
    "rr, cc, val = draw.line_aa(12, 12, 20, 50)\n",
    "img[rr, cc] = val\n",
    "\n",
    "# anti-aliased circle\n",
    "rr, cc, val = draw.circle_perimeter_aa(60, 40, 30)\n",
    "img[rr, cc] = val\n",
    "\n",
    "\n",
    "ax2.imshow(img, cmap=plt.cm.gray, interpolation='nearest')\n",
    "ax2.set_title('Anti-aliasing')\n",
    "ax2.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.exposure](https://scikit-image.org/docs/stable/api/skimage.exposure.html) - evaluating or changing the exposure of an image<a id='exposure'></a>\n",
    "\n",
    "One of the most common tools to evaluate exposure is the *histogram*, which plots the number of points which have a certain value against the values in order from lowest (dark) to highest (light).  The function `exposure.histogram` differs from `numpy.histogram` in that there is no rebinnning; each value along the x-axis is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "def plot_img_and_hist(image, axes, bins=256):\n",
    "    \"\"\"Plot an image along with its histogram and cumulative histogram.\n",
    "\n",
    "    \"\"\"\n",
    "    image = img_as_float(image)\n",
    "    ax_img, ax_hist = axes\n",
    "    ax_cdf = ax_hist.twinx()\n",
    "\n",
    "    # Display image\n",
    "    ax_img.imshow(image, cmap=plt.cm.gray)\n",
    "    ax_img.set_axis_off()\n",
    "\n",
    "    # Display histogram\n",
    "    ax_hist.hist(image.ravel(), bins=bins, histtype='step', color='black')\n",
    "    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n",
    "    ax_hist.set_xlabel('Pixel intensity')\n",
    "    ax_hist.set_xlim(0, 1)\n",
    "    ax_hist.set_yticks([])\n",
    "\n",
    "    # Display cumulative distribution\n",
    "    img_cdf, bins = exposure.cumulative_distribution(image, bins)\n",
    "    ax_cdf.plot(bins, img_cdf, 'r')\n",
    "    ax_cdf.set_yticks([])\n",
    "\n",
    "    return ax_img, ax_hist, ax_cdf\n",
    "\n",
    "\n",
    "# Load an example image\n",
    "img = data.moon()\n",
    "\n",
    "# Contrast stretching\n",
    "p2, p98 = np.percentile(img, (2, 98))\n",
    "img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "\n",
    "# Equalization\n",
    "img_eq = exposure.equalize_hist(img)\n",
    "\n",
    "# Adaptive Equalization\n",
    "img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
    "\n",
    "# Display results\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "axes = np.zeros((2, 4), dtype=np.object)\n",
    "axes[0, 0] = fig.add_subplot(2, 4, 1)\n",
    "for i in range(1, 4):\n",
    "    axes[0, i] = fig.add_subplot(2, 4, 1+i, \n",
    "                                 sharex=axes[0,0], sharey=axes[0,0])\n",
    "for i in range(0, 4):\n",
    "    axes[1, i] = fig.add_subplot(2, 4, 5+i)\n",
    "\n",
    "ax_img, ax_hist, ax_cdf = plot_img_and_hist(img, axes[:, 0])\n",
    "ax_img.set_title('Low contrast image')\n",
    "\n",
    "y_min, y_max = ax_hist.get_ylim()\n",
    "ax_hist.set_ylabel('Number of pixels')\n",
    "ax_hist.set_yticks(np.linspace(0, y_max, 5))\n",
    "\n",
    "ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_rescale, axes[:, 1])\n",
    "ax_img.set_title('Contrast stretch')\n",
    "\n",
    "ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_eq, axes[:, 2])\n",
    "ax_img.set_title('Histogram eq')\n",
    "\n",
    "ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_adapteq, axes[:, 3])\n",
    "ax_img.set_title('Adaptive eq')\n",
    "\n",
    "ax_cdf.set_ylabel('Fraction of total intensity')\n",
    "ax_cdf.set_yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "# prevent overlap of y-axis labels\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional examples available in the [example gallery](https://scikit-image.org/docs/stable/auto_examples/#manipulating-exposure-and-color-channels)\n",
    "\n",
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.feature](https://scikit-image.org/docs/stable/api/skimage.feature.html) - extract features from an image<a id='feature'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submodule presents a diverse set of tools to identify or extract certain features from images, including tools for\n",
    "\n",
    "* Edge detection\n",
    "  * `feature.canny`\n",
    "* Corner detection\n",
    "  * `feature.corner_kitchen_rosenfeld`\n",
    "  * `feature.corner_harris`\n",
    "  * `feature.corner_shi_tomasi`\n",
    "  * `feature.corner_foerstner`\n",
    "  * `feature.subpix`\n",
    "  * `feature.corner_moravec`\n",
    "  * `feature.corner_fast`\n",
    "  * `feature.corner_orientations`\n",
    "* Blob detection\n",
    "  * `feature.blob_dog`\n",
    "  * `feature.blob_doh`\n",
    "  * `feature.blob_log`\n",
    "* Texture\n",
    "  * `feature.greycomatrix`\n",
    "  * `feature.greycoprops`\n",
    "  * `feature.local_binary_pattern`\n",
    "  * `feature.multiblock_lbp`\n",
    "* Peak finding\n",
    "  * `feature.peak_local_max`\n",
    "* Object detction\n",
    "  * `feature.hog`\n",
    "  * `feature.match_template`\n",
    "* Stereoscopic depth estimation\n",
    "  * `feature.daisy`\n",
    "* Feature matching\n",
    "  * `feature.ORB`\n",
    "  * `feature.BRIEF`\n",
    "  * `feature.CENSURE`\n",
    "  * `feature.match_descriptors`\n",
    "  * `feature.plot_matches`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large submodule.  For brevity here is a short example illustrating ORB feature matching, and additional examples can be explored in the [online gallery](https://scikit-image.org/docs/stable/auto_examples/index.html#detection-of-features-and-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import transform as tf\n",
    "from skimage import feature\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Import the astronaut then warp/rotate the image\n",
    "img1 = rgb2gray(data.astronaut())\n",
    "img2 = tf.rotate(img1, 180)\n",
    "tform = tf.AffineTransform(scale=(1.3, 1.1), rotation=0.5,\n",
    "                           translation=(0, -200))\n",
    "img3 = tf.warp(img1, tform)\n",
    "\n",
    "# Build ORB extractor and extract features\n",
    "descriptor_extractor = feature.ORB(n_keypoints=200)\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img1)\n",
    "keypoints1 = descriptor_extractor.keypoints\n",
    "descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img2)\n",
    "keypoints2 = descriptor_extractor.keypoints\n",
    "descriptors2 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img3)\n",
    "keypoints3 = descriptor_extractor.keypoints\n",
    "descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "# Find matches between the extracted features\n",
    "matches12 = feature.match_descriptors(descriptors1, descriptors2, cross_check=True)\n",
    "matches13 = feature.match_descriptors(descriptors1, descriptors3, cross_check=True)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "feature.plot_matches(ax[0], img1, img2, keypoints1, keypoints2, matches12)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title(\"Original Image vs. Transformed Image\")\n",
    "\n",
    "feature.plot_matches(ax[1], img1, img3, keypoints1, keypoints3, matches13)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title(\"Original Image vs. Transformed Image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional feature detection and extraction examples available in the [online gallery](https://scikit-image.org/docs/stable/auto_examples/index.html#detection-of-features-and-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.filters](https://scikit-image.org/docs/stable/api/skimage.filters.html) - apply filters to an image<a id='filters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering applies whole-image modifications such as sharpening or blurring.  Thresholding methods also live in this submodule.\n",
    "\n",
    "Notable functions include (links to relevant gallery examples)\n",
    "\n",
    "* [Thresholding](https://scikit-image.org/docs/stable/auto_examples/applications/plot_thresholding.html)\n",
    "  * filters.threshold_* (multiple different functions with this prefix)\n",
    "  * skimage.filters.try_all_threshold to compare various methods\n",
    "* [Edge finding/enhancement](https://scikit-image.org/docs/stable/auto_examples/edges/plot_edge_filter.html)\n",
    "  * filters.sobel\n",
    "  * filters.prewitt\n",
    "  * filters.scharr\n",
    "  * filters.roberts\n",
    "  * filters.laplace\n",
    "  * filters.hessian\n",
    "* [Ridge filters](https://scikit-image.org/docs/stable/auto_examples/edges/plot_ridge_filter.html)\n",
    "  * filters.meijering\n",
    "  * filters.sato\n",
    "  * filters.frangi\n",
    "* Inverse filtering (see also [skimage.restoration](#restoration))\n",
    "  * filters.weiner\n",
    "  * filters.inverse\n",
    "* [Directional](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_gabor.html)\n",
    "  * filters.gabor\n",
    "* Blurring/denoising\n",
    "  * filters.gaussian\n",
    "  * filters.median\n",
    "* [Sharpening](https://scikit-image.org/docs/stable/auto_examples/filters/plot_unsharp_mask.html)\n",
    "  * filters.unsharp_mask\n",
    "* Define your own\n",
    "  * LPIFilter2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank filters\n",
    "There is a sub-submodule, `skimage.filters.rank`, which contains rank filters.  These filters are nonlinear and operate on the local histogram.\n",
    "\n",
    "To learn more about the rank filters, see the comprehensive [gallery example for rank filters](https://scikit-image.org/docs/stable/auto_examples/applications/plot_rank_filters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional feature detection and extraction examples available in the [online gallery](https://scikit-image.org/docs/stable/auto_examples/index.html#detection-of-features-and-objects).\n",
    "\n",
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.future](https://scikit-image.org/docs/stable/api/skimage.future.html) - stable code with unstable API<a id='future'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleeding edge features which work well, and will be moved from here into the main package in future releases.  However, on the way their API may change.\n",
    "\n",
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.graph](https://scikit-image.org/docs/stable/api/skimage.graph.html) - graph theory, minimum cost paths<a id='graph'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph theory.  Currently this submodule primarily deals with a constructed \"cost\" image, and how to find the minimum cost path through it, with constraints if desired.\n",
    "\n",
    "[The panorama tutorial lecture illustrates a real-world example.](./solutions/adv3_panorama-stitching-solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.io](https://scikit-image.org/docs/stable/api/skimage.io.html) - utilities to read and write images in various formats<a id='io'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading your image and writing the results back out.  There are multiple plugins available, which support multiple formats.  The most commonly used functions include\n",
    "\n",
    "* io.imread - Read an image to a numpy array.\n",
    "* io.imsave - Write an image to disk.\n",
    "* io.imread_collection - Read multiple images which match a common prefix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='measure'></a>[skimage.measure](https://scikit-image.org/docs/stable/api/skimage.measure.html) - measuring image or region properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple algorithms to label images, or obtain information about discrete regions of an image.  \n",
    "\n",
    "* Label an image\n",
    "  * measure.label\n",
    "  \n",
    " \n",
    "* In a labeled image (image with discrete regions identified by unique integers, as returned by `label`), find various properties of the labeled regions.  [**`regionprops` is extremely useful**](https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.html)\n",
    "  * measure.regionprops\n",
    "\n",
    "\n",
    "* Finding paths from a 2D image, or isosurfaces from a 3D image\n",
    "  * measure.find_contours\n",
    "  * measure.marching_cubes_lewiner\n",
    "  * measure.marching_cubes_classic\n",
    "  * measure.mesh_surface_area (surface area of 3D mesh from marching cubes)\n",
    "\n",
    "\n",
    "* Quantify the difference between two whole images (often used in denoising or restoration)\n",
    "  * measure.compare_*\n",
    "\n",
    "\n",
    "**RANDom Sample Consensus fitting (RANSAC)** - a powerful, robust approach to fitting a model to data.  It exists here because its initial use was for fitting shapes, but it can also fit transforms.\n",
    "* measure.ransac\n",
    "* measure.CircleModel\n",
    "* measure.EllipseModel\n",
    "* measure.LineModelND\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='morphology'></a>[skimage.morphology](https://scikit-image.org/docs/stable/api/skimage.morphology.html) - binary and grayscale morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological image processing is a collection of non-linear operations related to the shape or morphology of features in an image, such as boundaries, skeletons, etc. In any given technique, we probe an image with a small shape or template called a structuring element, which defines the region of interest or neighborhood around a pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology as morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "morph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Flood filling\n",
    "\n",
    "Flood fill is an algorithm to iteratively identify and/or change adjacent values in an image based on their similarity to an initial seed point. The conceptual analogy is the ‘paint bucket’ tool in many graphic editors.\n",
    "\n",
    "The `flood` function returns the binary mask of the flooded area.  `flood_fill` returns a modified image.  Both of these can be set with a `tolerance` keyword argument, within which the adjacent region will be filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will experiment a bit on the cameraman, turning his coat from dark to light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import morphology as morph\n",
    "\n",
    "cameraman = data.camera()\n",
    "\n",
    "# Change the cameraman's coat from dark to light (255).  The seed point is\n",
    "# chosen as (200, 100),\n",
    "light_coat = morph.flood_fill(cameraman, (200, 100), 255, tolerance=10)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(cameraman, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(light_coat, cmap=plt.cm.gray)\n",
    "ax[1].plot(100, 200, 'ro')  # seed point\n",
    "ax[1].set_title('After flood fill')\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Binary and grayscale morphology\n",
    "\n",
    "Here we outline the following basic morphological operations:\n",
    "\n",
    "1. Erosion\n",
    "2. Dilation\n",
    "3. Opening\n",
    "4. Closing\n",
    "5. White Tophat\n",
    "6. Black Tophat\n",
    "7. Skeletonize\n",
    "8. Convex Hull\n",
    "\n",
    "To get started, let’s load an image using `io.imread`. Note that morphology functions only work on gray-scale or binary images, so we set `as_gray=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.data import data_dir\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import io\n",
    "\n",
    "orig_phantom = img_as_ubyte(io.imread(os.path.join(data_dir, \"phantom.png\"),\n",
    "                                      as_gray=True))\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.imshow(orig_phantom, cmap=plt.cm.gray)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(original, filtered, filter_name):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4), sharex=True,\n",
    "                                   sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(filtered, cmap=plt.cm.gray)\n",
    "    ax2.set_title(filter_name)\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion\n",
    "\n",
    "Morphological `erosion` sets a pixel at (i, j) to the minimum over all pixels in the neighborhood centered at (i, j). *Erosion shrinks bright regions and enlarges dark regions.*\n",
    "\n",
    "The structuring element, `selem`, passed to erosion is a boolean array that describes this neighborhood. Below, we use `disk` to create a circular structuring element, which we use for most of the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology as morph\n",
    "\n",
    "selem = morph.disk(6)\n",
    "eroded = morph.erosion(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, eroded, 'erosion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n",
    "\n",
    "Morphological `dilation` sets a pixel at (i, j) to the maximum over all pixels in the neighborhood centered at (i, j). *Dilation enlarges bright regions and shrinks dark regions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated = morph.dilation(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, dilated, 'dilation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the white boundary of the image thickens, or gets dilated, as we increase the size of the disk. Also notice the decrease in size of the two black ellipses in the centre, and the thickening of the light grey circle in the center and the 3 patches in the lower part of the image.\n",
    "\n",
    "### Opening\n",
    "\n",
    "Morphological `opening` on an image is defined as an erosion followed by a dilation. *Opening can remove small bright spots (i.e. “salt”) and connect small dark cracks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opened = morph.opening(orig_phantom, selem)\n",
    "plot_comparison(orig_phantom, opened, 'opening')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since opening an image starts with an erosion operation, light regions that are smaller than the structuring element are removed. The dilation operation that follows ensures that light regions that are larger than the structuring element retain their original size. Notice how the light and dark shapes in the center their original thickness but the 3 lighter patches in the bottom get completely eroded. The size dependence is highlighted by the outer white ring: The parts of the ring thinner than the structuring element were completely erased, while the thicker region at the top retains its original thickness.\n",
    "\n",
    "### Closing\n",
    "\n",
    "Morphological `closing` on an image is defined as a dilation followed by an erosion. *Closing can remove small dark spots (i.e. “pepper”) and connect small bright cracks.*\n",
    "\n",
    "To illustrate this more clearly, let’s add a small crack to the white border:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = orig_phantom.copy()\n",
    "phantom[10:30, 200:210] = 0\n",
    "\n",
    "closed = morph.closing(phantom, selem)\n",
    "plot_comparison(phantom, closed, 'closing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since closing an image starts with an dilation operation, dark regions that are smaller than the structuring element are removed. The dilation operation that follows ensures that dark regions that are larger than the structuring element retain their original size. Notice how the white ellipses at the bottom get connected because of dilation, but other dark region retain their original sizes. Also notice how the crack we added is mostly removed.\n",
    "\n",
    "### White tophat\n",
    "\n",
    "The `white_tophat` of an image is defined as the image minus its morphological opening. *This operation returns the bright spots of the image that are smaller than the structuring element.*\n",
    "\n",
    "To make things interesting, we’ll add bright and dark spots to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = orig_phantom.copy()\n",
    "phantom[340:350, 200:210] = 255\n",
    "phantom[100:110, 200:210] = 0\n",
    "\n",
    "w_tophat = morph.white_tophat(phantom, selem)\n",
    "plot_comparison(phantom, w_tophat, 'white tophat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the 10-pixel wide white square is highlighted since it is smaller than the structuring element. Also, the thin, white edges around most of the ellipse are retained because they’re smaller than the structuring element, but the thicker region at the top disappears.\n",
    "\n",
    "### Black tophat\n",
    "\n",
    "The `black_tophat` of an image is defined as its morphological closing minus the original image. *This operation returns the dark spots of the image that are smaller than the structuring element.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tophat = morph.black_tophat(phantom, selem)\n",
    "plot_comparison(phantom, b_tophat, 'black tophat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the 10-pixel wide black square is highlighted since it is smaller than the structuring element.\n",
    "\n",
    "#### Duality\n",
    "\n",
    "As you should have noticed, many of these operations are simply the reverse of another operation. This duality can be summarized as follows:\n",
    "\n",
    "* Erosion <-> Dilation\n",
    "* Opening <-> Closing\n",
    "* White tophat <-> Black tophat\n",
    "\n",
    "\n",
    "### Skeletonize\n",
    "\n",
    "Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton. It is important to note that this is performed on binary images only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = io.imread(os.path.join(data_dir, \"horse.png\"), as_gray=True)\n",
    "\n",
    "sk = morph.skeletonize(horse == 0)\n",
    "plot_comparison(horse, sk, 'skeletonize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, this technique is used to thin the image to 1-pixel wide skeleton by applying thinning successively.\n",
    "\n",
    "### Convex hull\n",
    "\n",
    "The convex_hull_image is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image. Again note that this is also performed on binary images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull1 = morph.convex_hull_image(horse == 0)\n",
    "plot_comparison(horse, hull1, 'convex hull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.restoration](https://scikit-image.org/docs/stable/api/skimage.restoration.html) - restoration of an image<a id='restoration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submodule includes routines to restore images.  Currently these routines fall into four major categories.  Links lead to topical gallery examples.\n",
    "\n",
    "* [Reducing noise](https://scikit-image.org/docs/stable/auto_examples/filters/plot_denoise.html)\n",
    "  * restoration.denoise_*\n",
    "* [Deconvolution](https://scikit-image.org/docs/stable/auto_examples/filters/plot_deconvolution.html), or reversing a convolutional effect which applies to the entire image.  For example, lens correction.  This can be done [unsupervised](https://scikit-image.org/docs/stable/auto_examples/filters/plot_restoration.html).\n",
    "  * restoration.weiner\n",
    "  * restoration.unsupervised_weiner\n",
    "  * restoration.richardson_lucy\n",
    "* [Inpainting](https://scikit-image.org/docs/stable/auto_examples/filters/plot_inpaint.html), or filling in missing areas of an image\n",
    "  * restoration.inpaint_biharmonic\n",
    "* [Phase unwrapping](https://scikit-image.org/docs/stable/auto_examples/filters/plot_phase_unwrap.html)\n",
    "  * restoration.unwrap_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "restoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space to experiment with restoration techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='segmentation'></a>[skimage.segmentation](https://scikit-image.org/docs/stable/api/skimage.segmentation.html) - identification of regions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key image analysis tasks is identifying regions of interest.  These could be a person, an object, certain features of an animal, microscopic image, or stars.  Segmenting an image is the process of determining where these things you want are in your images.\n",
    "\n",
    "Segmentation has two overarching categories: Supervised and Unsupervised.\n",
    "\n",
    "**Supervised** - must provide some guidance (seed points or initial conditions)\n",
    "\n",
    "* segmentation.random_walker\n",
    "* segmentation.active_contour\n",
    "* segmentation.watershed\n",
    "* segmentation.flood_fill\n",
    "* segmentation.flood\n",
    "* some thresholding algorithms in `filters`\n",
    "\n",
    "\n",
    "**Unsupervised** - no human input\n",
    "\n",
    "* segmentation.slic\n",
    "* segmentation.felzenszwalb\n",
    "* segmentation.chan_vese\n",
    "* some thresholding algorithms in `filters`\n",
    "\n",
    "\n",
    "There is a [segmentation lecture](./4_segmentation.ipynb) ([and solution](./solutions/4_segmentation.ipynb)) you may peruse, as well as many [gallery examples](https://scikit-image.org/docs/stable/auto_examples/index.html#segmentation-of-objects) which illustrate all of these segmentation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.transform](https://scikit-image.org/docs/stable/api/skimage.transform.html) - transforms & warping<a id='transform'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submodule has multiple features which fall under the umbrella of transformations.\n",
    "\n",
    "Forward (`radon`) and inverse (`iradon`) radon transforms, as well as some variants (`iradon_sart`) and the finite versions of these transforms (`frt2` and `ifrt2`).  These are used for [reconstructing medical computed tomography (CT) images](https://scikit-image.org/docs/stable/auto_examples/transform/plot_radon_transform.html).\n",
    "\n",
    "Hough transforms for identifying lines, circles, and ellipses.\n",
    "\n",
    "Changing image size, shape, or resolution with `resize`, `rescale`, or `downscale_local_mean`.\n",
    "\n",
    "`warp`, and `warp_coordinates` which take an image or set of coordinates and translate them through one of the defined `*Transforms` in this submodule.  `estimate_transform` may be assist in estimating the parameters.\n",
    "\n",
    "[Numerous gallery examples are available](https://scikit-image.org/docs/stable/auto_examples/index.html#geometrical-transformations-and-registration) illustrating these functions.  [The panorama tutorial also includes warping](./solutions/adv3_panorama-stitching-solution.ipynb) via `SimilarityTransform` with parameter estimation via `measure.ransac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to the Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.util](https://scikit-image.org/docs/stable/api/skimage.util.html) - utility functions<a id='util'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are generally useful functions which have no definite other place in the package.\n",
    "\n",
    "`util.img_as_*` are convenience functions for datatype conversion.\n",
    "\n",
    "`util.invert` is a convenient way to invert any image, accounting for its datatype.\n",
    "\n",
    "`util.random_noise` is a comprehensive function to apply any amount of many different types of noise to images.  The seed may be set, resulting in pseudo-random noise for testing.\n",
    "\n",
    "`util.view_as_*` allows for overlapping views into the same memory array, which is useful for elegant local computations with minimal memory impact.\n",
    "\n",
    "`util.apply_parallel` uses Dask to apply a function across subsections of an image.  This can result in dramatic performance or memory improvements, but depending on the algorithm edge effects or lack of knowledge of the remainder of the image may result in unexpected results.\n",
    "\n",
    "`util.pad` and `util.crop` pads or crops the edges of images.  `util.pad` is now a direct wrapper for `numpy.pad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with tab completion\n",
    "util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room to experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to three-dimensional image processing\n",
    "\n",
    "Images are represented as `numpy` arrays. A single-channel, or grayscale, image is a 2D matrix of pixel intensities of shape `(row, column)`. We can construct a 3D volume as a series of 2D `planes`, giving 3D images the shape `(plane, row, column)`. Multichannel data adds a `channel` dimension in the final position containing color information. \n",
    "\n",
    "These conventions are summarized in the table below:\n",
    "\n",
    "\n",
    "|Image type|Coordinates|\n",
    "|:---|:---|\n",
    "|2D grayscale|(row, column)|\n",
    "|2D multichannel|(row, column, channel)|\n",
    "|3D grayscale|(plane, row, column)|\n",
    "|3D multichannel|(plane, row, column, channel)|\n",
    "\n",
    "Some 3D images are constructed with equal resolution in each dimension; e.g., a computer generated rendering of a sphere. Most experimental data captures one dimension at a lower resolution than the other two; e.g., photographing thin slices to approximate a 3D structure as a stack of 2D images. The distance between pixels in each dimension, called `spacing`, is encoded in a tuple and is accepted as a parameter by some `skimage` functions and can be used to adjust contributions to filters.\n",
    "\n",
    "## Input/Output and display\n",
    "\n",
    "Three dimensional data can be loaded with `skimage.io.imread`. The data for this tutorial was provided by the Allen Institute for Cell Science. It has been downsampled by a factor of 4 in the `row` and `column` dimensions to reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.imread(\"images/cells.tif\")\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type:  {data.dtype}\")\n",
    "print(f\"Data range: ({data.min()}, {data.max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between pixels was reported by the microscope used to image the cells. This `spacing` information will be used to adjust contributions to filters and helps decide when to apply operations planewise. We've chosen to normalize it to `1.0` in the `row` and `column` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The microscope reports the following spacing:\n",
    "original_spacing = np.array([0.2900000, 0.0650000, 0.0650000])\n",
    "print(f\"Microscope original spacing: {original_spacing}\")\n",
    "\n",
    "# We downsampled each slice 4x to make the data smaller\n",
    "rescaled_spacing = original_spacing * [1, 4, 4]\n",
    "print(f\"Microscope after rescaling images: {rescaled_spacing}\")\n",
    "\n",
    "# Normalize the spacing so that pixels are a distance of 1 apart\n",
    "spacing = rescaled_spacing / rescaled_spacing[2]\n",
    "print(f\"Microscope normalized spacing: {spacing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate (no need to read the following cell; execute to generate illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we all see the same thing\n",
    "np.random.seed(0)\n",
    "\n",
    "image = np.random.random((8, 8))\n",
    "image_rescaled = transform.downscale_local_mean(image, (4, 4))\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "ax0.imshow(image, cmap='gray')\n",
    "ax0.set_xticks([])\n",
    "ax0.set_yticks([])\n",
    "centers = np.indices(image.shape).reshape(2, -1).T\n",
    "ax0.plot(centers[:, 0], centers[:, 1], '.r')\n",
    "\n",
    "ax1.imshow(image_rescaled, cmap='gray')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "centers = np.indices(image_rescaled.shape).reshape(2, -1).T\n",
    "ax1.plot(centers[:, 0], centers[:, 1], '.r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our original data, let's try visualizing the image with `skimage.io.imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    io.imshow(data, cmap=\"gray\")\n",
    "except TypeError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skimage.io.imshow` can only display grayscale and RGB(A) 2D images. We can use `skimage.io.imshow` to visualize 2D planes. By fixing one axis, we can observe three different views of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (a, b, c) = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "sc.show_plane(a, data[32], title=\"Plane = 32\")\n",
    "sc.show_plane(b, data[:, 128, :], title=\"Row = 128\")\n",
    "sc.show_plane(c, data[:, :, 128], title=\"Column = 128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three-dimensional images can be viewed as a series of two-dimensional functions. The `display` helper function displays 30 planes of the provided image. By default, every other plane is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.slice_explorer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure\n",
    "\n",
    "`skimage.exposure` contains a number of functions for adjusting image contrast. These functions operate on pixel values. Generally, image dimensionality or pixel spacing does not need to be considered.\n",
    "\n",
    "[Gamma correction](https://en.wikipedia.org/wiki/Gamma_correction), also known as Power Law Transform, brightens or darkens an image. The function $O = I^\\gamma$ is applied to each pixel in the image. A `gamma < 1` will brighten an image, while a `gamma > 1` will darken an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_low_val = 0.5\n",
    "gamma_low = exposure.adjust_gamma(data, gamma=gamma_low_val)\n",
    "\n",
    "gamma_high_val = 1.5\n",
    "gamma_high = exposure.adjust_gamma(data, gamma=gamma_high_val)\n",
    "\n",
    "_, ((a, b, c), (d, e, f)) = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "sc.show_plane(a, data[32], title=\"Original\")\n",
    "sc.show_plane(b, gamma_low[32], title=\"Gamma = {}\".format(gamma_low_val))\n",
    "sc.show_plane(c, gamma_high[32], title=\"Gamma = {}\".format(gamma_high_val))\n",
    "\n",
    "sc.plot_hist(d, data)\n",
    "sc.plot_hist(e, gamma_low)\n",
    "sc.plot_hist(f, gamma_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Histogram equalization](https://en.wikipedia.org/wiki/Histogram_equalization) improves contrast in an image by redistributing pixel intensities. The most common pixel intensities are spread out, allowing areas of lower local contrast to gain a higher contrast. This may enhance background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = exposure.equalize_hist(data)\n",
    "\n",
    "sc.slice_explorer(equalized)\n",
    "\n",
    "_, ((a, b), (c, d)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\n",
    "\n",
    "plot_hist(a, data, title=\"Original\")\n",
    "plot_hist(b, equalized, title=\"Histogram equalization\")\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(data.ravel())\n",
    "c.plot(bins, cdf, \"r\")\n",
    "c.set_title(\"Original CDF\")\n",
    "\n",
    "cdf, bins = exposure.cumulative_distribution(equalized.ravel())\n",
    "d.plot(bins, cdf, \"r\")\n",
    "d.set_title(\"Histogram equalization CDF\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most experimental images are affected by salt and pepper noise. A few bright artifacts can decrease the relative intensity of the pixels of interest. A simple way to improve contrast is to clip the pixel values on the lowest and highest extremes. Clipping the darkest and brightest 0.5% of pixels will increase the overall contrast of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = stats.scoreatpercentile(data, (0.5, 99.5))\n",
    "\n",
    "clipped = exposure.rescale_intensity(\n",
    "    data, \n",
    "    in_range=(vmin, vmax), \n",
    "    out_range=np.float32\n",
    ")\n",
    "\n",
    "sc.slice_explorer(clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll call our dataset \"rescaled\" from here on\n",
    "# In this cell, you can choose any of the previous results\n",
    "# to continue working with.\n",
    "#\n",
    "# We'll use the `clipped` version\n",
    "#\n",
    "rescaled = clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
